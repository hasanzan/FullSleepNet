{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from utils import read_mesa_data, set_random_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score, roc_auc_score, average_precision_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from score import Score\n",
    "from models import ModelCLA\n",
    "from models.utils import cat_crossentropy_cut\n",
    "\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to edf files\n",
    "data_paths = glob.glob(\"mesa/data/*.edf\")\n",
    "\n",
    "# sampling freq\n",
    "fs = 256\n",
    "\n",
    "# batch size\n",
    "batch_size = 1\n",
    "\n",
    "# seed\n",
    "seed = 10\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features\n",
    "num_features = 1\n",
    "\n",
    "# create the model\n",
    "model = ModelCLA((2**23, num_features))\n",
    "\n",
    "# metrics\n",
    "AUPRC = tf.keras.metrics.AUC(curve='PR', name=\"AUPRC\")\n",
    "AUROC = tf.keras.metrics.AUC(curve='ROC', name=\"AUROC\")\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "        loss={'arousal': 'binary_crossentropy', 'stage': cat_crossentropy_cut},\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        metrics={'arousal': [AUPRC, AUROC], 'stage': \"accuracy\"},\n",
    "        loss_weights={'arousal': 1., 'stage': 1.}\n",
    "        )\n",
    "\n",
    "tensor_in = tf.random.uniform((1, 2**23, num_features))\n",
    "tensor_out = model(tensor_in)[0]\n",
    "out_shape = tensor_out.shape[1]\n",
    "r = tensor_in.shape[1] // tensor_out.shape[1]\n",
    "del tensor_in, tensor_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDS(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data_paths, r, batch_size, ds_type=\"train\", pad_len=2**22):\n",
    "        self.data_paths = data_paths\n",
    "        self.label_paths = [path.replace(\"data\", \"labels\").replace(\".edf\", \"-profusion.xml\").replace(\"mesa-sleep\", \"mesa-sleep-mesa-sleep\") for path in data_paths]\n",
    "        self.ds_type = ds_type\n",
    "        self.r = int(r)\n",
    "        self.pad_len = pad_len\n",
    "        self.batch_size = batch_size if ds_type != \"test\" else 1\n",
    "        self.shuffle = True if ds_type == \"train\" else False\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.batch_size * idx\n",
    "        end_idx = self.batch_size * (idx + 1) if self.batch_size * (idx + 1) <= len(self.data_paths) else len(self.data_paths)\n",
    "        data_batch = []\n",
    "        arousal_batch = []\n",
    "        stage_batch = []\n",
    "\n",
    "        for i in range(start_idx, end_idx):\n",
    "            data, arousals, stages = self.getitem(i)\n",
    "            data_batch.append(data)\n",
    "            arousal_batch.append(arousals)\n",
    "            stage_batch.append(stages)\n",
    "        return np.stack(data_batch, axis=0), (np.stack(arousal_batch, axis=0), to_categorical(np.stack(stage_batch, axis=0), num_classes=6))\n",
    "\n",
    "    \n",
    "    def getitem(self, idx):\n",
    "        # read data and label from disk\n",
    "        data, arousals, stages = read_mesa_data(self.data_paths[idx], self.label_paths[idx])\n",
    "\n",
    "        # augment\n",
    "        if self.ds_type == \"train\":\n",
    "            data, arousals, stages = self.augment_data(data, arousals, stages)  \n",
    "        \n",
    "        # if it is not test, resample labels. In this case pad data and labels\n",
    "        if self.ds_type == \"train\" or self.ds_type == \"val\":\n",
    "            data = np.pad(data, pad_width=((0, self.pad_len - data.shape[0]), (0, 0)))\n",
    "            arousals = np.pad(arousals, pad_width=((0, 2*self.pad_len - arousals.shape[0])))\n",
    "            stages = np.pad(stages, pad_width=((0, 2*self.pad_len - stages.shape[0])))\n",
    "            arousals = arousals[::2*self.r]\n",
    "            stages = stages[::2*self.r]\n",
    "        \n",
    "        # if it is test, pad data\n",
    "        if self.ds_type == \"test\":\n",
    "            try:\n",
    "                data = np.pad(data, pad_width=((0, self.pad_len - data.shape[0]), (0, 0)))\n",
    "            except:\n",
    "                print(data.shape, self.pad_len - data.shape[0])\n",
    "                raise ValueError(\"index can't contain negative values\")\n",
    "        \n",
    "        return data, arousals, stages\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            idx = np.random.permutation(len(self.data_paths))\n",
    "            self.data_paths = np.array(self.data_paths)[idx]\n",
    "            self.label_paths = np.array(self.label_paths)[idx]\n",
    "    \n",
    "    def augment_data(self, data, arousals, stages):\n",
    "        low = 0.9\n",
    "        high = 1.1\n",
    "        scale = np.random.uniform(low, high)\n",
    "        data = data * scale\n",
    "\n",
    "        return data, arousals, stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of files\n",
    "num_files = len(data_paths)\n",
    "\n",
    "# number of training files\n",
    "train_split = int(0.5 * num_files)\n",
    "# number of validation files\n",
    "val_split   = int(0.2 * num_files)\n",
    "# number of test files\n",
    "test_split  = num_files - train_split - val_split    \n",
    "# split files\n",
    "train_files, tmp = train_test_split(data_paths, test_size=val_split + test_split, random_state=seed)\n",
    "val_files, test_files = train_test_split(tmp, test_size=test_split, random_state=seed) \n",
    "\n",
    "# datasets\n",
    "ds_train = CreateDS(train_files, r, batch_size, ds_type=\"train\", pad_len=2**23)\n",
    "ds_val = CreateDS(val_files, r, batch_size, ds_type=\"val\", pad_len=2**23)\n",
    "ds_test = CreateDS(test_files, r, batch_size, ds_type=\"test\", pad_len=2**23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True, mode=\"max\")\n",
    "save = tf.keras.callbacks.CSVLogger(\"log_mesa.csv\", separator=',', append=False)\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(x=ds_train,\n",
    "                    validation_data=ds_val,\n",
    "                    epochs=200,\n",
    "                    callbacks=[early_stop, save],\n",
    "                    validation_freq=1,\n",
    "                    workers=10,\n",
    "                    use_multiprocessing=True) \n",
    "\n",
    "# plot \n",
    "plt.figure(dpi=200)\n",
    "perf = history.history[\"loss\"]\n",
    "perf_val = history.history[\"val_loss\"]\n",
    "ep = np.arange(1, len(perf)+1)\n",
    "\n",
    "plt.plot(ep, perf, label=\"Training\")\n",
    "plt.plot(ep, perf_val, label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model.save_weights(\"weights/mesa_model-C8-L3A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scorer\n",
    "scr = Score()\n",
    "\n",
    "# epoch length for stages\n",
    "epoch_len = fs * 30\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "y_true_all_a = []\n",
    "y_pred_all_a = []\n",
    "y_score_all_a = []\n",
    "\n",
    "# loop over files\n",
    "for i, (X, (y_true_a, y_true_s)) in enumerate(ds_test):\n",
    "    # get true labels\n",
    "    y_true_a = y_true_a.ravel()\n",
    "    y_true_s = y_true_s.argmax(axis=-1).ravel()\n",
    "    # make predictions\n",
    "    y_pred_a, y_pred_s = model.predict(X)\n",
    "    y_pred_a = y_pred_a.ravel()\n",
    "    y_pred_s = y_pred_s.argmax(axis=-1).ravel()\n",
    "\n",
    "    # over-sample predictions\n",
    "    y_pred_a = np.repeat(y_pred_a, 2*r)\n",
    "    y_pred_s = np.repeat(y_pred_s, 2*r)\n",
    "\n",
    "    # append or cut predictions\n",
    "    len_true = np.max(y_true_a.shape)\n",
    "    len_pred = np.max(y_pred_a.shape)\n",
    "\n",
    "    # cut if predictions are longer\n",
    "    if len_pred > len_true:     \n",
    "        y_pred_a = y_pred_a[:-(len_pred - len_true)]\n",
    "        y_pred_s = y_pred_s[:-(len_pred - len_true)]\n",
    "    \n",
    "    # make length of stage labels a multiple of epcoh_len and get epoch labels\n",
    "    y_true_s = y_true_s[:(y_true_s.shape[0]//epoch_len)*epoch_len].reshape((-1, epoch_len))\n",
    "    y_true_s = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=y_true_s)\n",
    "\n",
    "    y_pred_s = y_pred_s[:(y_pred_s.shape[0]//epoch_len)*epoch_len].reshape((-1, epoch_len))\n",
    "    y_pred_s = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=y_pred_s)\n",
    "    \n",
    "    # epoch-based arousal metrics\n",
    "    # make length of arousal labels a multiple of epcoh_len and get epoch labels\n",
    "    y_true_a2 = y_true_a[:(y_true_a.shape[0]//epoch_len)*epoch_len].reshape((-1, epoch_len))\n",
    "    y_true_a2 = np.apply_along_axis(lambda x: np.max(x), axis=1, arr=y_true_a2)\n",
    "    \n",
    "    y_score_a2 = y_pred_a[:(y_pred_a.shape[0]//epoch_len)*epoch_len].reshape((-1, epoch_len))\n",
    "    y_score_a2 = np.apply_along_axis(lambda x: np.mean(x), axis=1, arr=y_score_a2)\n",
    "    \n",
    "    y_pred_a2 = y_pred_a >= 0.5\n",
    "    y_pred_a2 = y_pred_a2[:(y_pred_a2.shape[0]//epoch_len)*epoch_len].reshape((-1, epoch_len))\n",
    "    y_pred_a2 = np.apply_along_axis(lambda x: np.max(x), axis=1, arr=y_pred_a2)\n",
    "\n",
    "    # remove undefined epochs\n",
    "    y_pred_s = y_pred_s[y_true_s != 5]\n",
    "    y_true_s = y_true_s[y_true_s != 5]    \n",
    "\n",
    "    # append predictions \n",
    "    y_true_all += y_true_s.tolist()\n",
    "    y_pred_all += y_pred_s.tolist()\n",
    "    \n",
    "    y_true_all_a += y_true_a2.tolist()\n",
    "    y_pred_all_a += y_pred_a2.tolist()\n",
    "    y_score_all_a += y_score_a2.tolist()\n",
    "    \n",
    "    # get score\n",
    "    scr.score_record(y_true_a, y_pred_a, record_name=str(i))\n",
    "\n",
    "# print final scores\n",
    "print(\"------------- Results for Arousals -------------\")\n",
    "print(f\"AUPRC: {scr.gross_auprc():.3f}, AUROC: {scr.gross_auroc():.3f}\")\n",
    "\n",
    "print(\"\\n------------- Results for Arousals (Epoch-Based) -------------\")\n",
    "print(classification_report(y_true_all_a, y_pred_all_a, digits=3, ))\n",
    "print(f\"AUPRC: {average_precision_score(y_true_all_a, y_score_all_a):.3f}, AUROC: {roc_auc_score(y_true_all_a, y_score_all_a):.3f}\")\n",
    "print(f\"Cohen's Kappa: {cohen_kappa_score(y_true_all_a, y_pred_all_a):.3f}\")\n",
    "\n",
    "# print stage scores\n",
    "print(\"\\n-------------Results for Sleep Stages -------------\")\n",
    "print(classification_report(y_true_all, y_pred_all, digits=3))\n",
    "print(f\"Cohen's Kappa: {cohen_kappa_score(y_true_all, y_pred_all):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "631926b96873d2007fbe5ba14401cac9ce99e5e76ded0bffc18be58f3fefa61b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
